{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FERapp.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arifuddinatif/face-emotion-recognition/blob/main/FERapp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afQC2r71PQMg"
      },
      "source": [
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyjK9FsFPiKZ"
      },
      "source": [
        "# !pip install streamlit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An3Tsidhc7mw",
        "outputId": "b2508ba5-703b-4ab3-d82a-d439b432644c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pc8HAVFWf-B0"
      },
      "source": [
        "!pip install flask-ngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZVe1PM3hkHz",
        "outputId": "91fcbb2f-1050-4c0e-ed1a-ecce8a830aed"
      },
      "source": [
        "!pip install pyngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-5.1.0.tar.gz (745 kB)\n",
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |▉                               | 20 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 81 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████                            | 92 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 256 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 266 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 276 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 286 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 296 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 307 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 317 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 327 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 337 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 348 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 358 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 368 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 378 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 389 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 399 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 409 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 419 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 430 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 440 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 450 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 460 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 471 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 481 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 491 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 501 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 512 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 522 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 532 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 542 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 552 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 563 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 573 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 583 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 593 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 604 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 614 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 624 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 634 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 645 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 655 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 665 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 675 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 686 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 696 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 706 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 716 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 727 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 737 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 745 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (3.13)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.1.0-py3-none-any.whl size=19006 sha256=c870c57ef4b88869aaaeb603be4caf0b2744b0d8ddd25835328856ab2e4845cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/e6/af/ccf6598ecefecd44104069371795cb9b3afbcd16987f6ccfb3\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To98q39bufSw",
        "outputId": "65feb818-57b9-41c3-9bb0-84f18d7e8619"
      },
      "source": [
        "! ngrok authtoken 1xFnYtGBLJtnVaSpGLwo9qhCOZu_4gRapCrF7aHizdUiCkR1r"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKxfioEmcq5k"
      },
      "source": [
        "# %%writefile app.py\n",
        "from flask import Flask,render_template,Response\n",
        "from flask_ngrok import run_with_ngrok\n",
        "import cv2\n",
        "import numpy as np\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "\n",
        "\n",
        "app=Flask(__name__,template_folder='/content/drive/MyDrive/Capstone Project 5/templates')\n",
        "# run_with_ngrok(app)\n",
        "# # import portpicker\n",
        "# # port = portpicker.pick_unused_port()\n",
        "# # from google.colab import output\n",
        "# # output.serve_kernel_port_as_window(port)\n",
        "\n",
        "# # from gevent.pywsgi import WSGIServer\n",
        "# # host='localhost'\n",
        "# # app_server = WSGIServer((host, port), app)\n",
        "# # app_server.serve_forever()\n",
        "\n",
        "# ## # JavaScript to properly create our live video stream using our webcam as input\n",
        "# def video_stream():\n",
        "#   js = Javascript('''\n",
        "#     var video;\n",
        "#     var div = null;\n",
        "#     var stream;\n",
        "#     var captureCanvas;\n",
        "#     var imgElement;\n",
        "#     var labelElement;\n",
        "    \n",
        "#     var pendingResolve = null;\n",
        "#     var shutdown = false;\n",
        "    \n",
        "#     function removeDom() {\n",
        "#        stream.getVideoTracks()[0].stop();\n",
        "#        video.remove();\n",
        "#        div.remove();\n",
        "#        video = null;\n",
        "#        div = null;\n",
        "#        stream = null;\n",
        "#        imgElement = null;\n",
        "#        captureCanvas = null;\n",
        "#        labelElement = null;\n",
        "#     }\n",
        "    \n",
        "#     function onAnimationFrame() {\n",
        "#       if (!shutdown) {\n",
        "#         window.requestAnimationFrame(onAnimationFrame);\n",
        "#       }\n",
        "#       if (pendingResolve) {\n",
        "#         var result = \"\";\n",
        "#         if (!shutdown) {\n",
        "#           captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "#           result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "#         }\n",
        "#         var lp = pendingResolve;\n",
        "#         pendingResolve = null;\n",
        "#         lp(result);\n",
        "#       }\n",
        "#     }\n",
        "    \n",
        "#     async function createDom() {\n",
        "#       if (div !== null) {\n",
        "#         return stream;\n",
        "#       }\n",
        "\n",
        "#       div = document.createElement('div');\n",
        "#       div.style.border = '2px solid black';\n",
        "#       div.style.padding = '3px';\n",
        "#       div.style.width = '100%';\n",
        "#       div.style.maxWidth = '600px';\n",
        "#       document.body.appendChild(div);\n",
        "      \n",
        "#       const modelOut = document.createElement('div');\n",
        "#       modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "#       labelElement = document.createElement('span');\n",
        "#       labelElement.innerText = 'No data';\n",
        "#       labelElement.style.fontWeight = 'bold';\n",
        "#       modelOut.appendChild(labelElement);\n",
        "#       div.appendChild(modelOut);\n",
        "           \n",
        "#       video = document.createElement('video');\n",
        "#       video.style.display = 'block';\n",
        "#       video.width = div.clientWidth - 6;\n",
        "#       video.setAttribute('playsinline', '');\n",
        "#       video.onclick = () => { shutdown = true; };\n",
        "#       stream = await navigator.mediaDevices.getUserMedia(\n",
        "#           {video: { facingMode: \"environment\"}});\n",
        "#       div.appendChild(video);\n",
        "\n",
        "#       imgElement = document.createElement('img');\n",
        "#       imgElement.style.position = 'absolute';\n",
        "#       imgElement.style.zIndex = 1;\n",
        "#       imgElement.onclick = () => { shutdown = true; };\n",
        "#       div.appendChild(imgElement);\n",
        "      \n",
        "#       const instruction = document.createElement('div');\n",
        "#       instruction.innerHTML = \n",
        "#           '<span style=\"color: red; font-weight: bold;\">' +\n",
        "#           'When finished, click here or on the video to stop this demo</span>';\n",
        "#       div.appendChild(instruction);\n",
        "#       instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "#       video.srcObject = stream;\n",
        "#       await video.play();\n",
        "\n",
        "#       captureCanvas = document.createElement('canvas');\n",
        "#       captureCanvas.width = 640; //video.videoWidth;\n",
        "#       captureCanvas.height = 480; //video.videoHeight;\n",
        "#       window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "#       return stream;\n",
        "#     }\n",
        "#     async function stream_frame(label, imgData) {\n",
        "#       if (shutdown) {\n",
        "#         removeDom();\n",
        "#         shutdown = false;\n",
        "#         return '';\n",
        "#       }\n",
        "\n",
        "#       var preCreate = Date.now();\n",
        "#       stream = await createDom();\n",
        "      \n",
        "#       var preShow = Date.now();\n",
        "#       if (label != \"\") {\n",
        "#         labelElement.innerHTML = label;\n",
        "#       }\n",
        "            \n",
        "#       if (imgData != \"\") {\n",
        "#         var videoRect = video.getClientRects()[0];\n",
        "#         imgElement.style.top = videoRect.top + \"px\";\n",
        "#         imgElement.style.left = videoRect.left + \"px\";\n",
        "#         imgElement.style.width = videoRect.width + \"px\";\n",
        "#         imgElement.style.height = videoRect.height + \"px\";\n",
        "#         imgElement.src = imgData;\n",
        "#       }\n",
        "      \n",
        "#       var preCapture = Date.now();\n",
        "#       var result = await new Promise(function(resolve, reject) {\n",
        "#         pendingResolve = resolve;\n",
        "#       });\n",
        "#       shutdown = false;\n",
        "      \n",
        "#       return {'create': preShow - preCreate, \n",
        "#               'show': preCapture - preShow, \n",
        "#               'capture': Date.now() - preCapture,\n",
        "#               'img': result};\n",
        "#     }\n",
        "#     ''')\n",
        "\n",
        "#   display(js)\n",
        "\n",
        "def video_frame(label, bbox):\n",
        "    data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "    return data\n",
        "\n",
        "def js_to_image(js_reply):\n",
        "    image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "    jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "    img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "    return img\n",
        "\n",
        "def bbox_to_bytes(bbox_array):\n",
        "    # convert array into PIL image\n",
        "    bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "    iobuf = io.BytesIO()\n",
        "    # format bbox into png for return\n",
        "    bbox_PIL.save(iobuf, format='png')\n",
        "    # format return string\n",
        "    bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "    return bbox_bytes\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))\n",
        "\n",
        "\n",
        "\n",
        "colour_cycle = ((255, 0, 0), (0, 255, 0), (0, 0, 255), (230, 230, 250))\n",
        "\n",
        "from keras.models import model_from_json\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def load_model(path):\n",
        "\n",
        "\tjson_file = open(path + 'model.json', 'r')\n",
        "\tloaded_model_json = json_file.read()\n",
        "\tjson_file.close()\n",
        "\tmodel = model_from_json(loaded_model_json)\n",
        "\tmodel.load_weights(path + \"model.h5\")\n",
        "\tprint(\"Loaded model from disk\")\n",
        "\treturn model\n",
        "\t\n",
        "def predict_emotion(gray, x, y, w, h):\n",
        "\tface = np.expand_dims(np.expand_dims(np.resize(gray[y:y+w, x:x+h]/255.0, (48, 48)),-1), 0)\n",
        "\tprediction = model.predict([face])\n",
        "\treturn(int(np.argmax(prediction)), round(max(prediction[0])*100, 2))\n",
        " \n",
        "\n",
        "path = \"/content/drive/MyDrive/Capstone Project 5/\"\n",
        "model = load_model(path)\n",
        "\n",
        "fcc_path = \"Tools/haarcascade_frontalface_alt.xml\"\n",
        "faceCascade = cv2.CascadeClassifier(fcc_path)\n",
        "emotion_dict = {0: \"Angry\", 1: \"Disgust\", 2: \"Fear\", 3: \"Happy\", 4: \"Sad\", 5: \"Surprise\", 6: \"Neutral\"}\n",
        "colour_cycle = ((255, 0, 0), (0, 255, 0), (0, 0, 255), (230, 230, 250))\n",
        "\n",
        "cam = cv2.videoCapture() #pycharm entry\n",
        "\n",
        " \n",
        "def gen_frames():\n",
        "    label_html = 'Capturing...'\n",
        "    count = 0\n",
        "\n",
        "    while True:\n",
        "        success,frame = camera.read()\n",
        "        bbox = ''\n",
        "        js_reply = video_frame(label_html,bbox)\n",
        "        if not js_reply:\n",
        "            print('Invalid')\n",
        "        else:\n",
        "            img = js_to_image(js_reply[\"img\"])\n",
        "            # create transparent overlay for bounding box\n",
        "            bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "            # grayscale image for face detection\n",
        "            gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "            #  get face region coordinates\n",
        "            faces = face_cascade.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=5,minSize=(30, 30))\n",
        "            # # get face bounding box for overlay\n",
        "    # for (x,y,w,h) in faces:\n",
        "    #   bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "\n",
        "    # bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    # # convert overlay of bbox into bytes\n",
        "    # bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # # update bbox so next frame gets new overlay\n",
        "    # bbox = bbox_bytes\n",
        "\n",
        "            for (count,(x, y, w, h)) in enumerate(faces):   \n",
        "                colour = colour_cycle[int(count%len(colour_cycle))]\n",
        "                bbox_array = cv2.rectangle(bbox_array, (x, y), (x+w, y+h), colour, 2)\n",
        "                bbox_array = cv2.line(bbox_array, (x+5, y+h+5),(x+100, y+h+5), colour, 20)\n",
        "                bbox_array = cv2.putText(bbox_array, \"Face #\"+str(count+1), (x+5, y+h+11), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), lineType=cv2.LINE_AA)\n",
        "                bbox_array = cv2.line(bbox_array, (x+8, y),(x+150, y), colour, 20)\n",
        "                emotion_id, confidence = predict_emotion(gray, x, y, w, h)\n",
        "                emotion = emotion_dict[emotion_id]\n",
        "                bbox_array = cv2.putText(bbox_array, emotion + \": \" + str(confidence) + \"%\" , (x+20, y+5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), lineType=cv2.LINE_AA)\n",
        "      # out.write(img)\n",
        "                bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "     # convert overlay of bbox into bytes\n",
        "                bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # update bbox so next frame gets new overlay\n",
        "                bbox = bbox_bytes\n",
        "                ret,buffer=cv2.imencode('.jpg',img)\n",
        "                frame=buffer.tobytes()\n",
        "      \n",
        "      # cv2_imshow(img)\n",
        "      # print(\"Frame number: \" + str(counter))\n",
        "      # counter = counter+1\n",
        "      \n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                print('Invalid')\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "@app.route('/video_feed')\n",
        "def video_feed():\n",
        "    return Response(gen_frames(),mimetype='multipart/x-mixed-replace;boundary=frame')\n",
        "if __name__ == '__main__':\n",
        "  app.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d79bqsMM9_4G"
      },
      "source": [
        "# %%writefile FERAPP.py\n",
        "# import streamlit as st\n",
        "# st.title(\"WebApp\")\n",
        "# run  = st.checkbox('Run')\n",
        "# FRAME_WINDOW = st.image([])\n",
        "# while run:\n",
        "#   # import dependencies\n",
        "#   from IPython.display import display, Javascript, Image\n",
        "#   from google.colab.output import eval_js\n",
        "#   from base64 import b64decode, b64encode\n",
        "#   import cv2\n",
        "#   import numpy as np\n",
        "#   import PIL\n",
        "#   import io\n",
        "#   import html\n",
        "#   import time\n",
        "\n",
        "\n",
        "#   # function to convert the JavaScript object into an OpenCV image\n",
        "#   def js_to_image(js_reply):\n",
        "#   # decode base64 image\n",
        "#     image_bytes = b64decode(js_reply.split(',')[1])\n",
        "#   # convert bytes to numpy array\n",
        "#     jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "#   # decode numpy array into OpenCV BGR image\n",
        "#     img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "#     return img\n",
        "\n",
        "# # function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "#   def bbox_to_bytes(bbox_array):\n",
        "#     \"\"\"\n",
        "#     Params:\n",
        "#             bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "#     Returns:\n",
        "#           bytes: Base64 image byte string\n",
        "#     \"\"\"\n",
        "#   # convert array into PIL image\n",
        "#     bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "#     iobuf = io.BytesIO()\n",
        "#   # format bbox into png for return\n",
        "#     bbox_PIL.save(iobuf, format='png')\n",
        "#   # format return string\n",
        "#     bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "#     return bbox_bytes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # initialize the Haar Cascade face detection model\n",
        "#   face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))\n",
        "\n",
        "\n",
        "  \n",
        "#   def video_frame(label, bbox):\n",
        "#     data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label,bbox))\n",
        "#     return data\n",
        "#   colour_cycle = ((255, 0, 0), (0, 255, 0), (0, 0, 255), (230, 230, 250))\n",
        "#   from keras.models import model_from_json\n",
        "#   import numpy as np\n",
        "#   import cv2\n",
        "\n",
        "#   def load_model(path):\n",
        "\n",
        "# \t  json_file = open(path + 'model.json', 'r')\n",
        "# \t  loaded_model_json = json_file.read()\n",
        "# \t  json_file.close()\n",
        "\t\n",
        "# \t  model = model_from_json(loaded_model_json)\n",
        "# \t  model.load_weights(path + \"model.h5\")\n",
        "# \t  print(\"Loaded model from disk\")\n",
        "# \t  return model\n",
        "\t\n",
        "#   def predict_emotion(gray, x, y, w, h):\n",
        "# \t  face = np.expand_dims(np.expand_dims(np.resize(gray[y:y+w, x:x+h]/255.0, (48, 48)),-1), 0)\n",
        "# \t  prediction = model.predict([face])\n",
        "\n",
        "# \t  return(int(np.argmax(prediction)), round(max(prediction[0])*100, 2))\n",
        "   \n",
        "#   path = \"/content/drive/MyDrive/Capstone Project 5/\"\n",
        "#   model = load_model(path)\n",
        "\n",
        "#   fcc_path = \"Tools/haarcascade_frontalface_alt.xml\"\n",
        "#   faceCascade = cv2.CascadeClassifier(fcc_path)\n",
        "#   emotion_dict = {0: \"Angry\", 1: \"Disgust\", 2: \"Fear\", 3: \"Happy\", 4: \"Sad\", 5: \"Surprise\", 6: \"Neutral\"}\n",
        "#   colour_cycle = ((255, 0, 0), (0, 255, 0), (0, 0, 255), (230, 230, 250))\n",
        "#   from google.colab.patches import cv2_imshow\n",
        "#   from IPython.display import clear_output\n",
        "\n",
        "# # start streaming video from webcam\n",
        "#   video_stream()\n",
        "# # label for video\n",
        "#   label_html = 'Capturing...'\n",
        "# # initialze bounding box to empty\n",
        "#   bbox = ''\n",
        "#   count = 0 \n",
        "#   counter = 1\n",
        "  \n",
        "#   while True:\n",
        "#       js_reply = video_frame(label_html,bbox)\n",
        "#       if not js_reply:\n",
        "#           break\n",
        "\n",
        "#     # convert JS response to OpenCV Image\n",
        "#       img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "#     # create transparent overlay for bounding box\n",
        "#       bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "#     # grayscale image for face detection\n",
        "#       gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "#     # get face region coordinates\n",
        "#       faces = face_cascade.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=5,minSize=(30, 30))\n",
        "#     # # get face bounding box for overlay\n",
        "#     # for (x,y,w,h) in faces:\n",
        "#     #   bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "\n",
        "#     # bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "#     # # convert overlay of bbox into bytes\n",
        "#     # bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "#     # # update bbox so next frame gets new overlay\n",
        "#     # bbox = bbox_bytes\n",
        "\n",
        "#       for (count,(x, y, w, h)) in enumerate(faces):\n",
        "#         colour = colour_cycle[int(count%len(colour_cycle))]\n",
        "#         bbox_array = cv2.rectangle(bbox_array, (x, y), (x+w, y+h), colour, 2)\n",
        "#         bbox_array = cv2.line(bbox_array, (x+5, y+h+5),(x+100, y+h+5), colour, 20)\n",
        "#         bbox_array = cv2.putText(bbox_array, \"Face #\"+str(count+1), (x+5, y+h+11), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), lineType=cv2.LINE_AA)\n",
        "#         bbox_array = cv2.line(bbox_array, (x+8, y),(x+150, y), colour, 20)\n",
        "#         emotion_id, confidence = predict_emotion(gray, x, y, w, h)\n",
        "#         emotion = emotion_dict[emotion_id]\n",
        "#         bbox_array = cv2.putText(bbox_array, emotion + \": \" + str(confidence) + \"%\" , (x+20, y+5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), lineType=cv2.LINE_AA)\n",
        "#       # out.write(img)\n",
        "#         bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "#      # convert overlay of bbox into bytes\n",
        "#         bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "#     # update bbox so next frame gets new overlay\n",
        "#         bbox = bbox_bytes\n",
        "      \n",
        "#       # cv2_imshow(img)\n",
        "#       # print(\"Frame number: \" + str(counter))\n",
        "#       # counter = counter+1\n",
        "      \n",
        "#       if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "#         break;\n",
        "#   out.release()\n",
        "# # webcam.release()\n",
        "#   cv2.destroyAllWindows()\n",
        "# else:\n",
        "#   st.write('Stopped')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9iAKW1tjcZD"
      },
      "source": [
        "# %%writefile app.py\n",
        "# from flask import Flask, render_template, request\n",
        "# import cv2\n",
        "# from keras.models import load_model\n",
        "# import numpy as np\n",
        "# from flask_ngrok import run_with_ngrok\n",
        "# app = Flask(__name__,template_folder='/content/drive/MyDrive/Capstone Project 5/templates')\n",
        "# run_with_ngrok(app)\n",
        "# import portpicker\n",
        "# port = portpicker.pick_unused_port()\n",
        "# from google.colab import output\n",
        "# output.serve_kernel_port_as_window(port)\n",
        "\n",
        "# from gevent.pywsgi import WSGIServer\n",
        "# host='localhost'\n",
        "# app_server = WSGIServer((host, port), app)\n",
        "# app_server.serve_forever()\n",
        "\n",
        "# app.config['SEND_FILE_MAX_AGE_DEFAULT'] = 1\n",
        "\n",
        "\n",
        "# @app.route('/')\n",
        "# def index():\n",
        "#     return render_template('index.html')\n",
        "\n",
        "# @app.route('/after', methods=['GET', 'POST'])\n",
        "# def after():\n",
        "#     img = request.files['file1']\n",
        "\n",
        "#     img.save('static/file.jpg')\n",
        "\n",
        "#     ####################################\n",
        "#     img1 = cv2.imread('static/file.jpg')\n",
        "#     gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "#     cascade = cv2.CascadeClassifier('haarcascade_frontalface_alt2.xml')\n",
        "#     faces = cascade.detectMultiScale(gray, 1.1, 3)\n",
        "\n",
        "#     for x,y,w,h in faces:\n",
        "#         cv2.rectangle(img1, (x,y), (x+w, y+h), (0,255,0), 2)\n",
        "\n",
        "#         cropped = img1[y:y+h, x:x+w]\n",
        "\n",
        "#     cv2.imwrite('static/after.jpg', img1)\n",
        "\n",
        "#     try:\n",
        "#         cv2.imwrite('static/cropped.jpg', cropped)\n",
        "\n",
        "#     except:\n",
        "#         pass\n",
        "\n",
        "#     #####################################\n",
        "\n",
        "#     try:\n",
        "#         image = cv2.imread('static/cropped.jpg', 0)\n",
        "#     except:\n",
        "#         image = cv2.imread('static/file.jpg', 0)\n",
        "\n",
        "#     image = cv2.resize(image, (48,48))\n",
        "\n",
        "#     image = image/255.0\n",
        "\n",
        "#     image = np.reshape(image, (1,48,48,1))\n",
        "\n",
        "#     model = load_model('/content/drive/MyDrive/Capstone Project 5/model.h5')\n",
        "\n",
        "#     prediction = model.predict(image)\n",
        "\n",
        "#     label_map =   ['Anger','Neutral' , 'Fear', 'Happy', 'Sad', 'Surprise']\n",
        "\n",
        "#     prediction = np.argmax(prediction)\n",
        "\n",
        "#     final_prediction = label_map[prediction]\n",
        "\n",
        "#     return render_template('After.html', data=final_prediction)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     app.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EElixpqjuoag",
        "outputId": "8d89287f-227e-4645-e626-d78506640063"
      },
      "source": [
        "# !nohup flask run --server.port 80 app.py &"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXiDXrWouv0O",
        "outputId": "59fe635b-dc07-4956-dbd8-6b7ed03862a5"
      },
      "source": [
        "# from pyngrok import ngrok\n",
        "\n",
        "# url = ngrok.connect(port=8501)\n",
        "# url"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"http://e633-34-125-209-26.ngrok.io\" -> \"http://localhost:80\">"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPIxLbMTvAnh"
      },
      "source": [
        "# ngrok.kill()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k20lWpUefeO_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b8c8bfc-a4bf-4d9e-dcdd-f31d0a6c0eda"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://raw.githubusercontent.com/computationalcore/introduction-to-opencv/master/assets/haarcascade_frontalface_default.xml \\\n",
        "    -O haarcascade_frontalface_default.xml\n",
        "\n",
        "\n",
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "\n",
        "\n",
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# initialize the Haar Cascade face detection model\n",
        "face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))\n",
        "\n",
        "\n",
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-09 07:45:44--  https://raw.githubusercontent.com/computationalcore/introduction-to-opencv/master/assets/haarcascade_frontalface_default.xml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 930127 (908K) [text/plain]\n",
            "Saving to: ‘haarcascade_frontalface_default.xml’\n",
            "\n",
            "\r          haarcasca   0%[                    ]       0  --.-KB/s               \rhaarcascade_frontal 100%[===================>] 908.33K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-09-09 07:45:44 (19.6 MB/s) - ‘haarcascade_frontalface_default.xml’ saved [930127/930127]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSDrD_fAhQbK"
      },
      "source": [
        "colour_cycle = ((255, 0, 0), (0, 255, 0), (0, 0, 255), (230, 230, 250))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGDFtqZPhUBZ"
      },
      "source": [
        "from keras.models import model_from_json\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def load_model(path):\n",
        "\n",
        "\tjson_file = open(path + 'model.json', 'r')\n",
        "\tloaded_model_json = json_file.read()\n",
        "\tjson_file.close()\n",
        "\t\n",
        "\tmodel = model_from_json(loaded_model_json)\n",
        "\tmodel.load_weights(path + \"model.h5\")\n",
        "\tprint(\"Loaded model from disk\")\n",
        "\treturn model\n",
        "\t\n",
        "def predict_emotion(gray, x, y, w, h):\n",
        "\tface = np.expand_dims(np.expand_dims(np.resize(gray[y:y+w, x:x+h]/255.0, (48, 48)),-1), 0)\n",
        "\tprediction = model.predict([face])\n",
        "\n",
        "\treturn(int(np.argmax(prediction)), round(max(prediction[0])*100, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQDKYhQmhWiu",
        "outputId": "cb8427db-b8be-463f-ed07-d0f8cc0b6575"
      },
      "source": [
        "path = \"/content/drive/MyDrive/Capstone Project 5/\"\n",
        "model = load_model(path)\n",
        "\n",
        "fcc_path = \"Tools/haarcascade_frontalface_alt.xml\"\n",
        "faceCascade = cv2.CascadeClassifier(fcc_path)\n",
        "emotion_dict = {0: \"Angry\", 1: \"Disgust\", 2: \"Fear\", 3: \"Happy\", 4: \"Sad\", 5: \"Surprise\", 6: \"Neutral\"}\n",
        "colour_cycle = ((255, 0, 0), (0, 255, 0), (0, 0, 255), (230, 230, 250))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model from disk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QDOaU5nqOqo",
        "outputId": "a3732f24-4a0e-4f98-81ac-ce47ce1b6d11"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://raw.githubusercontent.com/computationalcore/introduction-to-opencv/master/assets/haarcascade_frontalface_default.xml \\\n",
        "    -O haarcascade_frontalface_default.xml\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-09 08:25:10--  https://raw.githubusercontent.com/computationalcore/introduction-to-opencv/master/assets/haarcascade_frontalface_default.xml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 930127 (908K) [text/plain]\n",
            "Saving to: ‘haarcascade_frontalface_default.xml’\n",
            "\n",
            "haarcascade_frontal 100%[===================>] 908.33K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-09-09 08:25:10 (19.6 MB/s) - ‘haarcascade_frontalface_default.xml’ saved [930127/930127]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VfxPjZItjUh"
      },
      "source": [
        "import os\n",
        "\n",
        "def convert_avi_to_mp4(avi_file_path, output_name):\n",
        "    os.popen(\"ffmpeg -i '{input}' -ac 2 -b:v 2000k -c:a aac -c:v libx264 -b:a 160k -vprofile high -bf 0 -strict experimental -f mp4 '{output}.mp4'\".format(input = avi_file_path, output = output_name))\n",
        "    return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSeoUI4FkdBT"
      },
      "source": [
        "cap = cv2.VideoCapture(\"/content/drive/MyDrive/Capstone Project 5/Videos/pexels-gabby-k-5273028.mp4\")\n",
        "\n",
        "while True:\n",
        "    # Find haar cascade to draw bounding box around face\n",
        "    ret, frame = cap.read()\n",
        "    frame = cv2.resize(frame, (1280, 720))\n",
        "    if not ret:\n",
        "        break\n",
        "    face_detector = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # detect faces available on camera\n",
        "    num_faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
        "\n",
        "    # take each face available on the camera and Preprocess it\n",
        "    for (x, y, w, h) in num_faces:\n",
        "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (0, 255, 0), 4)\n",
        "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
        "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
        "\n",
        "        # predict the emotions\n",
        "        emotion_prediction = model.predict(cropped_img)\n",
        "        maxindex = int(np.argmax(emotion_prediction))\n",
        "        cv2.putText(frame, emotion_dict[maxindex], (x+5, y-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
        "        cv2.imwrite('kang'+str(i)+'.jpg',frame)\n",
        "        i+=1\n",
        "    import glob\n",
        "    img_array = []\n",
        "    for filename in glob.glob('/content/*.jpg'):\n",
        "      img = cv2.imread(filename)\n",
        "      height, width, layers = img.shape\n",
        "      size = (width,height)\n",
        "      img_array.append(img)\n",
        "    out = cv2.VideoWriter('project.avi',cv2.VideoWriter_fourcc(*'DIVX'), 15, size)\n",
        "    for i in range(len(img_array)):\n",
        "      out.write(img_array[i])\n",
        "    out.release()\n",
        "\n",
        "    convert_avi_to_mp4('/content/sample_data/project.avi','project.mp4')\n",
        "    \n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgGWWkQHpTja"
      },
      "source": [
        "import cv2\n",
        " \n",
        "# Opens the Video file\n",
        "cap= cv2.VideoCapture('/content/drive/MyDrive/Capstone Project 5/Videos/pexels-gabby-k-5273028.mp4')\n",
        "i=0\n",
        "while(cap.isOpened()):\n",
        "    ret, frame = cap.read()\n",
        "    if ret == False:\n",
        "        break\n",
        "    cv2.imwrite('kang'+str(i)+'.jpg',frame)\n",
        "    i+=1\n",
        " \n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuQy5Ymnr2Uf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}